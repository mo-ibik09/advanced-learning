{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e05d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 22:46:15,290 - INFO - Using device: cpu\n",
      "2025-05-21 22:46:15,291 - INFO - Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-05-21 22:46:15,295 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Enhanced RAG Chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 22:46:20,136 - INFO - Loading LLM: gpt2-medium\n",
      "Device set to use cpu\n",
      "2025-05-21 22:47:21,864 - INFO - Loading document: C:\\Users\\acer\\Desktop\\advanced learning\\Docker+for+Beginners-Mumshad+Mannambeth.pdf\n",
      "2025-05-21 22:47:24,592 - INFO - Creating embeddings...\n",
      "Embedding chunks:   0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b026bc66844dbb9f327b0ad7401601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:   7%|‚ñã         | 1/14 [00:00<00:07,  1.76it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a882e4aefe43d1832c4dc0e734eee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:  14%|‚ñà‚ñç        | 2/14 [00:00<00:05,  2.29it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6daca4973a40485c9544787f84a1c97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:  21%|‚ñà‚ñà‚ñè       | 3/14 [00:01<00:03,  2.81it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce344b8eb9b346baa6c317f0498ef975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:  29%|‚ñà‚ñà‚ñä       | 4/14 [00:01<00:03,  3.22it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146b17e29fb9478f94cb019dd08ed7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:  36%|‚ñà‚ñà‚ñà‚ñå      | 5/14 [00:01<00:02,  3.63it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069d6642b6574b45aa35d825d008cbf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6/14 [00:01<00:02,  3.26it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34550d0f39d54d30aa77f41c07056c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7/14 [00:02<00:02,  3.17it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a9ed45b30444e9a17f77bf48a89aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8/14 [00:02<00:01,  3.14it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03066f873b7043d58d8fa1b683a12982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9/14 [00:02<00:01,  3.19it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361f2dcbb66a46a8b6846b771a3e104d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10/14 [00:03<00:01,  3.01it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69e04a78bfb405d97ffa9c19c0dfb66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11/14 [00:03<00:00,  3.04it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82473519b96f4264aaa7761b9d1d939a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12/14 [00:03<00:00,  3.09it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1cc36846da40948db501dcd255d073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13/14 [00:04<00:00,  3.41it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884099ad684d4fbbba69ad95628f086e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:04<00:00,  3.08it/s]\n",
      "2025-05-21 22:47:29,143 - INFO - Building FAISS index...\n",
      "2025-05-21 22:47:29,146 - INFO - Successfully processed 14 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö Enhanced RAG Chatbot ready!\n",
      "üìä Loaded 14 chunks from your document(s)\n",
      "üí¨ Ask questions about the content. Type 'exit' to quit, 'help' for commands.\n",
      "\n",
      "\n",
      "ü§ñ Analyzing... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb56a09d13e45b9b575bb9b3b009d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "ü§ñ Bot: \"A virtual machine running as an application that runs inside of another process.\"\n",
      "\n",
      "ü§ñ Analyzing... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde4479b4d33419b8fb9260eb5c643d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (1024). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "2025-05-21 22:48:41,711 - ERROR - Error generating response: index out of range in self\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "ü§ñ Bot: I found relevant information: docker f o r b e g i n n e r s MUMSHAD MANNAMBETH w w w . k o d e k l o u d . c o m Objectives What are Containers? What is Docker? Why do you need it? What can it do? Run Docker Containers Create a D...\n",
      "\n",
      "ü§ñ Analyzing... "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8093cca0fc14487bb3170a6e7df80b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "ü§ñ Bot: It's very powerful as an application orchestrator that allows us not just to build applications but also manage them through our infrastructure like we would any other system running software such services etc. The fact of this is because there isn¬¥t one single solution out here yet which will allow all these different scenarios together without having too much overhead when they come into play at once ‚Äì especially if your workload has many components involved including networking/storage systems‚Ä¶ And even though some people might argue about how \"unnecessary\" virtual machines really are compared against physical ones, I think most users who use their computers regularly already have enough computing power available within those devices anyway due both hardware resources used by computer itself plus network bandwidth usage.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import textwrap\n",
    "from typing import List, Dict\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ImprovedRAGChatbot:\n",
    "    def __init__(self, \n",
    "                 embedding_model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "                 llm_model_name: str = 'gpt2-medium',  # Better model for coherent responses\n",
    "                 chunk_size: int = 250,\n",
    "                 chunk_overlap: int = 40):\n",
    "        \"\"\"\n",
    "        Initialize the improved RAG chatbot with better response quality.\n",
    "        \"\"\"\n",
    "        # Detect device\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load embedding model\n",
    "        logger.info(f\"Loading embedding model: {embedding_model_name}\")\n",
    "        self.embedder = SentenceTransformer(embedding_model_name, device=self.device)\n",
    "        \n",
    "        # Load language model with better configuration\n",
    "        logger.info(f\"Loading LLM: {llm_model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "        \n",
    "        # Configure tokenizer properly\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        # Initialize the model with better parameters\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_model_name,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        if self.device == \"cuda\":\n",
    "            self.model = self.model.to(self.device)\n",
    "            \n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device=0 if self.device == \"cuda\" else -1,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "        )\n",
    "        \n",
    "        # RAG parameters\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.chunks = []\n",
    "        self.index = None\n",
    "        self.metadata = {}\n",
    "        \n",
    "        # Conversation history\n",
    "        self.history = []\n",
    "        \n",
    "    def load_document(self, file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Load and process document with enhanced text cleaning.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading document: {file_path}\")\n",
    "        \n",
    "        # Simple extension-based handling\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        if ext == '.txt':\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "        elif ext == '.pdf':\n",
    "            try:\n",
    "                import PyPDF2\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    text = \"\"\n",
    "                    for page in reader.pages:\n",
    "                        page_text = page.extract_text()\n",
    "                        if page_text:\n",
    "                            text += page_text + \"\\n\"\n",
    "            except ImportError:\n",
    "                logger.error(\"PyPDF2 not installed. Install it using 'pip install PyPDF2'\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error reading PDF: {e}\")\n",
    "                return\n",
    "        elif ext in ['.docx', '.doc']:\n",
    "            try:\n",
    "                import docx\n",
    "                doc = docx.Document(file_path)\n",
    "                text = \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "            except ImportError:\n",
    "                logger.error(\"python-docx not installed. Install it using 'pip install python-docx'\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error reading Word document: {e}\")\n",
    "                return\n",
    "        else:\n",
    "            logger.error(f\"Unsupported file format: {ext}. Supported formats: .txt, .pdf, .docx\")\n",
    "            return\n",
    "            \n",
    "        if not text.strip():\n",
    "            logger.error(\"No text content found in the document\")\n",
    "            return\n",
    "            \n",
    "        # Process the text\n",
    "        self._process_text(text, source=file_path)\n",
    "    \n",
    "    def load_text(self, text: str, source: str = \"custom_text\") -> None:\n",
    "        \"\"\"\n",
    "        Load and process text directly with validation.\n",
    "        \"\"\"\n",
    "        if not text.strip():\n",
    "            logger.error(\"Empty text provided\")\n",
    "            return\n",
    "            \n",
    "        logger.info(f\"Loading text from: {source}\")\n",
    "        self._process_text(text, source)\n",
    "        \n",
    "    def _process_text(self, text: str, source: str) -> None:\n",
    "        \"\"\"\n",
    "        Process text into chunks with improved cleaning and build the vector index.\n",
    "        \"\"\"\n",
    "        # Enhanced text cleaning\n",
    "        text = self._clean_text(text)\n",
    "        \n",
    "        if len(text.split()) < 10:\n",
    "            logger.error(\"Text is too short to process effectively\")\n",
    "            return\n",
    "        \n",
    "        # Chunk the text with overlap\n",
    "        chunks_with_metadata = self._chunk_text(text, source)\n",
    "        \n",
    "        if not chunks_with_metadata:\n",
    "            logger.error(\"No valid chunks created from the text\")\n",
    "            return\n",
    "        \n",
    "        # Store chunks and metadata\n",
    "        self.chunks = [item[\"text\"] for item in chunks_with_metadata]\n",
    "        self.metadata = {i: {\n",
    "            \"source\": item[\"source\"],\n",
    "            \"chunk_id\": i,\n",
    "            \"word_count\": len(item[\"text\"].split())\n",
    "        } for i, item in enumerate(chunks_with_metadata)}\n",
    "        \n",
    "        # Create embeddings with progress bar\n",
    "        logger.info(\"Creating embeddings...\")\n",
    "        embeddings = []\n",
    "        for chunk in tqdm(self.chunks, desc=\"Embedding chunks\"):\n",
    "            try:\n",
    "                embedding = self.embedder.encode(chunk, convert_to_tensor=False)\n",
    "                embeddings.append(embedding)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to embed chunk: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not embeddings:\n",
    "            logger.error(\"No embeddings created\")\n",
    "            return\n",
    "            \n",
    "        # Build FAISS index\n",
    "        logger.info(\"Building FAISS index...\")\n",
    "        dimension = embeddings[0].shape[0]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(np.array(embeddings))\n",
    "        \n",
    "        logger.info(f\"Successfully processed {len(self.chunks)} chunks\")\n",
    "        \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Enhanced text cleaning for better processing.\n",
    "        \"\"\"\n",
    "        # Remove excessive whitespace and normalize\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove special characters that might interfere\n",
    "        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\-\\(\\)]', ' ', text)\n",
    "        \n",
    "        # Fix common OCR issues\n",
    "        text = re.sub(r'\\b([a-z])([A-Z])', r'\\1 \\2', text)  # Split camelCase\n",
    "        \n",
    "        # Remove very short lines (likely OCR artifacts)\n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = [line.strip() for line in lines if len(line.strip()) > 3]\n",
    "        \n",
    "        return ' '.join(cleaned_lines).strip()\n",
    "        \n",
    "    def _chunk_text(self, text: str, source: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Improved text chunking with overlap and quality filtering.\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        \n",
    "        if len(words) < self.chunk_size:\n",
    "            # If text is shorter than chunk size, return as single chunk\n",
    "            return [{\n",
    "                \"text\": text,\n",
    "                \"source\": source,\n",
    "            }] if len(words) >= 20 else []  # Minimum 20 words per chunk\n",
    "        \n",
    "        chunks_with_metadata = []\n",
    "        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):\n",
    "            chunk_words = words[i:i + self.chunk_size]\n",
    "            \n",
    "            # Skip very small chunks\n",
    "            if len(chunk_words) < 20:\n",
    "                continue\n",
    "                \n",
    "            chunk_text = ' '.join(chunk_words)\n",
    "            \n",
    "            # Quality filter: skip chunks that are mostly numbers or special characters\n",
    "            if self._is_quality_chunk(chunk_text):\n",
    "                chunks_with_metadata.append({\n",
    "                    \"text\": chunk_text,\n",
    "                    \"source\": source,\n",
    "                })\n",
    "            \n",
    "        return chunks_with_metadata\n",
    "    \n",
    "    def _is_quality_chunk(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a chunk contains meaningful content.\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        if len(words) < 10:\n",
    "            return False\n",
    "            \n",
    "        # Check if chunk has reasonable word/number ratio\n",
    "        word_count = len([w for w in words if w.isalpha()])\n",
    "        return word_count / len(words) > 0.5  # At least 50% actual words\n",
    "        \n",
    "    def retrieve_relevant_chunks(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve most relevant chunks with improved scoring.\n",
    "        \"\"\"\n",
    "        if not self.index:\n",
    "            logger.error(\"No index available. Load a document first.\")\n",
    "            return []\n",
    "            \n",
    "        # Get embeddings for the query\n",
    "        try:\n",
    "            query_embedding = self.embedder.encode([query], convert_to_tensor=False)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to embed query: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Search the index\n",
    "        D, I = self.index.search(np.array(query_embedding), min(top_k, len(self.chunks)))\n",
    "        \n",
    "        # Return relevant chunks with metadata\n",
    "        results = []\n",
    "        for idx, distance in zip(I[0], D[0]):\n",
    "            if idx < 0 or idx >= len(self.chunks):\n",
    "                continue\n",
    "                \n",
    "            # Only include chunks with reasonable similarity (lower distance = higher similarity)\n",
    "            if distance < 2.0:  # Threshold for relevance\n",
    "                results.append({\n",
    "                    \"text\": self.chunks[idx],\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"score\": float(distance)\n",
    "                })\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    def _detect_repetitive_response(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Detect if the response is repetitive or low quality.\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) < 10:\n",
    "            return True\n",
    "            \n",
    "        words = text.split()\n",
    "        if len(words) < 5:\n",
    "            return True\n",
    "            \n",
    "        # Check for excessive repetition\n",
    "        unique_words = set(words)\n",
    "        repetition_ratio = len(unique_words) / len(words)\n",
    "        \n",
    "        if repetition_ratio < 0.3:  # Less than 30% unique words\n",
    "            return True\n",
    "            \n",
    "        # Check for common repetitive patterns\n",
    "        text_lower = text.lower()\n",
    "        repetitive_phrases = [\n",
    "            \"it is used for\", \"it can be used\", \"docker is\", \"docker can\"\n",
    "        ]\n",
    "        \n",
    "        phrase_count = sum(text_lower.count(phrase) for phrase in repetitive_phrases)\n",
    "        if phrase_count > 3:  # Too many repetitive phrases\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "        \n",
    "    def generate_response(self, query: str, context_chunks: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Generate a high-quality response using retrieved context.\n",
    "        \"\"\"\n",
    "        if not context_chunks:\n",
    "            return \"I couldn't find relevant information in the document to answer your question.\"\n",
    "        \n",
    "        # Format the context more effectively\n",
    "        context_parts = []\n",
    "        for i, chunk in enumerate(context_chunks[:2]):  # Use top 2 chunks to avoid overwhelming\n",
    "            context_parts.append(f\"Reference {i+1}: {chunk['text']}\")\n",
    "        \n",
    "        context_text = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Create a more structured prompt\n",
    "        prompt = f\"\"\"Based on the following information, provide a clear and concise answer to the question.\n",
    "\n",
    "Information:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a focused answer based only on the information above. If the information doesn't fully answer the question, say so.\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Generate response with improved parameters\n",
    "        try:\n",
    "            # Use better generation parameters\n",
    "            response = self.generator(\n",
    "                prompt,\n",
    "                max_new_tokens=150,\n",
    "                do_sample=True,\n",
    "                temperature=0.3,  # Lower temperature for more focused responses\n",
    "                top_p=0.85,\n",
    "                repetition_penalty=1.2,  # Reduce repetition\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "            \n",
    "            # Extract the answer\n",
    "            full_response = response[0]['generated_text']\n",
    "            \n",
    "            # Clean the response\n",
    "            if \"Answer:\" in full_response:\n",
    "                answer = full_response.split(\"Answer:\")[-1].strip()\n",
    "            else:\n",
    "                # Fallback: take text after the prompt\n",
    "                answer = full_response[len(prompt):].strip()\n",
    "            \n",
    "            # Post-process the answer\n",
    "            answer = self._post_process_answer(answer)\n",
    "            \n",
    "            # Check for quality issues\n",
    "            if self._detect_repetitive_response(answer):\n",
    "                # Provide a fallback response with direct context\n",
    "                best_chunk = context_chunks[0]\n",
    "                return f\"Based on the document, here's what I found about your question: {best_chunk['text'][:300]}...\"\n",
    "            \n",
    "            # Track conversation history\n",
    "            self.history.append({\"query\": query, \"response\": answer})\n",
    "            \n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response: {e}\")\n",
    "            # Fallback to context-based response\n",
    "            if context_chunks:\n",
    "                return f\"I found relevant information: {context_chunks[0]['text'][:200]}...\"\n",
    "            return \"I'm sorry, I encountered an error while generating a response.\"\n",
    "    \n",
    "    def _post_process_answer(self, answer: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and improve the generated answer.\n",
    "        \"\"\"\n",
    "        if not answer:\n",
    "            return \"I couldn't generate a clear answer based on the available information.\"\n",
    "        \n",
    "        # Remove incomplete sentences at the end\n",
    "        sentences = re.split(r'[.!?]+', answer)\n",
    "        \n",
    "        # Keep only complete sentences\n",
    "        complete_sentences = []\n",
    "        for sentence in sentences[:-1]:  # Exclude last (potentially incomplete) sentence\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence) > 10 and sentence[0].isupper():\n",
    "                complete_sentences.append(sentence)\n",
    "        \n",
    "        # If we have complete sentences, use them\n",
    "        if complete_sentences:\n",
    "            result = '. '.join(complete_sentences) + '.'\n",
    "        else:\n",
    "            # Fallback to original answer, but truncate at reasonable length\n",
    "            result = answer[:300] if len(answer) > 300 else answer\n",
    "        \n",
    "        # Clean up formatting\n",
    "        result = re.sub(r'\\s+', ' ', result).strip()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def ask(self, query: str, top_k: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        End-to-end process to answer a question with improved error handling.\n",
    "        \"\"\"\n",
    "        if not self.index:\n",
    "            return \"Please load a document first using the load_document() method.\"\n",
    "            \n",
    "        if not query.strip():\n",
    "            return \"Please provide a question to answer.\"\n",
    "            \n",
    "        # Retrieve relevant chunks\n",
    "        relevant_chunks = self.retrieve_relevant_chunks(query, top_k)\n",
    "        \n",
    "        if not relevant_chunks:\n",
    "            return \"I couldn't find relevant information in the document to answer your question. Try rephrasing your question or check if the document contains information about this topic.\"\n",
    "            \n",
    "        # Generate and return response\n",
    "        return self.generate_response(query, relevant_chunks)\n",
    "        \n",
    "    def chat(self) -> None:\n",
    "        \"\"\"\n",
    "        Enhanced interactive chat interface.\n",
    "        \"\"\"\n",
    "        if not self.index:\n",
    "            print(\"‚ùå No document loaded. Please load a document first.\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\nüìö Enhanced RAG Chatbot ready!\")\n",
    "        print(f\"üìä Loaded {len(self.chunks)} chunks from your document(s)\")\n",
    "        print(\"üí¨ Ask questions about the content. Type 'exit' to quit, 'help' for commands.\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"üßë You: \").strip()\n",
    "                \n",
    "                if query.lower() == 'exit':\n",
    "                    print(\"üëã Goodbye!\")\n",
    "                    break\n",
    "                elif query.lower() == 'help':\n",
    "                    print(\"\\nüìã Available commands:\")\n",
    "                    print(\"  'exit' - Exit the chat\")\n",
    "                    print(\"  'help' - Show this help message\")\n",
    "                    print(\"  'sources' - Show sources for the last answer\")\n",
    "                    print(\"  'stats' - Show document statistics\")\n",
    "                    continue\n",
    "                elif query.lower() == 'sources':\n",
    "                    if not self.history:\n",
    "                        print(\"üìù No previous answers to show sources for.\")\n",
    "                    else:\n",
    "                        print(\"\\nüìö Sources for the last answer:\")\n",
    "                        last_query = self.history[-1][\"query\"]\n",
    "                        sources = self.retrieve_relevant_chunks(last_query, 3)\n",
    "                        for i, source in enumerate(sources, 1):\n",
    "                            print(f\"\\nüìÑ Source {i}: {source['metadata']['source']}\")\n",
    "                            print(f\"üéØ Relevance score: {source['score']:.3f}\")\n",
    "                            print(f\"üìñ Preview: {textwrap.shorten(source['text'], width=150)}\")\n",
    "                    continue\n",
    "                elif query.lower() == 'stats':\n",
    "                    print(f\"\\nüìä Document Statistics:\")\n",
    "                    print(f\"  üìÑ Total chunks: {len(self.chunks)}\")\n",
    "                    total_words = sum(meta['word_count'] for meta in self.metadata.values())\n",
    "                    print(f\"  üìù Total words: {total_words:,}\")\n",
    "                    print(f\"  üî¢ Average words per chunk: {total_words // len(self.chunks)}\")\n",
    "                    sources = set(meta['source'] for meta in self.metadata.values())\n",
    "                    print(f\"  üìö Source documents: {len(sources)}\")\n",
    "                    continue\n",
    "                elif not query:\n",
    "                    continue\n",
    "                    \n",
    "                # Get and display answer\n",
    "                print(\"\\nü§ñ Analyzing... \", end=\"\", flush=True)\n",
    "                answer = self.ask(query)\n",
    "                print(\"Done!\")\n",
    "                \n",
    "                # Format and display the answer\n",
    "                print(f\"\\nü§ñ Bot: {answer}\")\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nüëã Goodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Chat error: {e}\")\n",
    "                print(f\"\\nü§ñ Bot: I encountered an error. Please try again.\")\n",
    "\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Create the improved chatbot\n",
    "        print(\"üöÄ Initializing Enhanced RAG Chatbot...\")\n",
    "        chatbot = ImprovedRAGChatbot()\n",
    "        \n",
    "        # Check if document path is provided\n",
    "        document_path = input(\"üìÅ Enter the path to your document (or press Enter for demo): \").strip()\n",
    "        \n",
    "        if document_path:\n",
    "            if os.path.exists(document_path):\n",
    "                chatbot.load_document(document_path)\n",
    "                if chatbot.index:\n",
    "                    # Start interactive chat\n",
    "                    chatbot.chat()\n",
    "                else:\n",
    "                    print(\"‚ùå Failed to process the document. Please check the file format and content.\")\n",
    "            else:\n",
    "                print(f\"‚ùå File not found: {document_path}\")\n",
    "        else:\n",
    "            # Demo mode with sample text\n",
    "            sample_text = \"\"\"\n",
    "            Docker is a containerization platform that enables developers to package applications and their dependencies into lightweight, portable containers. \n",
    "            \n",
    "            These containers include everything needed to run an application: code, runtime, system tools, libraries, and settings. Docker containers are isolated from each other and the host system, making them secure and consistent across different environments.\n",
    "            \n",
    "            Key benefits of Docker include:\n",
    "            - Portability: Containers run consistently across development, testing, and production environments\n",
    "            - Efficiency: Containers share the host OS kernel, making them more resource-efficient than virtual machines\n",
    "            - Scalability: Easy to scale applications horizontally by spinning up multiple container instances\n",
    "            - Version control: Docker images can be versioned and tracked\n",
    "            - Microservices architecture: Perfect for breaking down monolithic applications into smaller, manageable services\n",
    "            \n",
    "            Docker uses a client-server architecture with the Docker daemon managing containers, images, networks, and volumes. The Docker CLI provides commands to interact with the daemon.\n",
    "            \n",
    "            Common Docker commands include:\n",
    "            - docker run: Create and start a new container\n",
    "            - docker build: Build an image from a Dockerfile\n",
    "            - docker pull: Download an image from a registry\n",
    "            - docker ps: List running containers\n",
    "            - docker stop: Stop a running container\n",
    "            \"\"\"\n",
    "            \n",
    "            print(\"üéØ Loading demo content about Docker...\")\n",
    "            chatbot.load_text(sample_text, \"Docker Documentation Demo\")\n",
    "            \n",
    "            if chatbot.index:\n",
    "                print(\"‚úÖ Demo loaded successfully!\")\n",
    "                chatbot.chat()\n",
    "            else:\n",
    "                print(\"‚ùå Failed to process demo content.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Startup error: {e}\")\n",
    "        print(f\"‚ùå Error starting chatbot: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
